â¸»


# ğŸ“„ RAG Chatbot (FastAPI + Streamlit + LangChain)

A **Retrieval-Augmented Generation (RAG) chatbot** built using **FastAPI** for the backend, **Streamlit** for the frontend, **LangChain** for orchestration, **ChromaDB** for vector storage, and **Groq LLMs** for inference.

---

## ğŸ§  Architecture Overview

- **Backend**: FastAPI  
- **Frontend**: Streamlit  
- **LLM Provider**: Groq  
- **Vector Store**: ChromaDB (persistent, volume-mounted)  
- **Embeddings**: sentence-transformers/all-MiniLM-L12-v2  
- **PDF Ingestion**: PyPDFLoader + Recursive Text Splitting  
- **Python Version**: 3.12.4  

---

## ğŸ“ Project Structure

RAG-CHATBOT/
â”œâ”€â”€ client/                 # Streamlit frontend
â”‚   â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ app.py
â”‚   â””â”€â”€ config.py
â”‚
â”œâ”€â”€ server/                 # FastAPI backend
â”‚   â”œâ”€â”€ modules/
â”‚   â”‚   â”œâ”€â”€ load_vectorstore.py
â”‚   â”‚   â”œâ”€â”€ llm.py
â”‚   â”‚   â””â”€â”€ query_handlers.py
â”‚   â”œâ”€â”€ chroma_store/       # Vector DB (Docker volume)
â”‚   â”œâ”€â”€ uploaded_pdfs/      # Uploaded PDFs (runtime)
â”‚   â””â”€â”€ main.py
â”‚
â”œâ”€â”€ logger.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”œâ”€â”€ .env
â”œâ”€â”€ .gitignore
â””â”€â”€ Dockerfile

---

## ğŸš€ Features

- Upload PDFs and build a vector database
- Semantic search using embeddings
- Context-aware answers using Groq LLMs
- Persistent vector storage using Docker volumes
- Clean separation of frontend and backend
- Centralized logging and error handling

---

## ğŸ”‘ Environment Variables

Create a `.env` file or pass via Docker:

```env
GROQ_API_KEY=your_groq_api_key_here


â¸»

ğŸ“¦ Installation (Local)

Create Virtual Environment

python -m venv myenv
source myenv/bin/activate

Install Dependencies

pip install -r requirements.txt


â¸»

â–¶ï¸ Running Locally

Start Backend (FastAPI)

cd server
uvicorn main:app --reload

Backend runs at:

http://127.0.0.1:8000

Start Frontend (Streamlit)

cd client
streamlit run app.py

Frontend runs at:

http://127.0.0.1:8501


â¸»

ğŸ§ª API Endpoints

Endpoint	Method	Description
/upload_pdfs/	POST	Upload PDF documents
/ask/	POST	Ask questions to the RAG agent
/test	GET	Health check


â¸»

ğŸ³ Docker Deployment

Build Image

docker build -t rag-chatbot .

Run Container

docker run -p 8000:8000 -p 8501:8501 \
  -e GROQ_API_KEY=your_key_here \
  -v chroma_data:/app/server/chroma_store \
  rag-chatbot

Why Docker Volume?
	â€¢	Keeps embeddings persistent across restarts
	â€¢	Avoids rebuilding vector database every run

â¸»

ğŸ§¹ .dockerignore (Important)

Ignored during Docker build:
	â€¢	Virtual environments
	â€¢	Cache files
	â€¢	.env
	â€¢	chroma_store
	â€¢	Uploaded PDFs

This keeps images secure and lightweight.

â¸»

ğŸ“š Key Libraries Used
	â€¢	LangChain
	â€¢	ChromaDB
	â€¢	Sentence Transformers
	â€¢	Groq SDK
	â€¢	FastAPI
	â€¢	Streamlit
	â€¢	PyPDF

â¸»

âš ï¸ Notes
	â€¢	OpenAI models are disabled by default
	â€¢	Groq models are recommended for inference
	â€¢	Ensure Docker volume is mounted for persistence

â¸»

ğŸ“Œ Future Improvements
	â€¢	Streaming responses
	â€¢	Authentication
	â€¢	Multi-user chat history
	â€¢	Docker Compose setup
	â€¢	Cloud deployment

â¸»

---