---

# ğŸ“„ RAG Chatbot

**FastAPI Â· Streamlit Â· LangChain Â· ChromaDB Â· Groq**

A **Retrieval-Augmented Generation (RAG) chatbot** built with **FastAPI** for the backend, **Streamlit** for the frontend, **LangChain** for orchestration, **ChromaDB** for vector storage, and **Groq LLMs** for fast inference.

---

## ğŸ§  Architecture Overview

* **Backend**: FastAPI
* **Frontend**: Streamlit
* **LLM Provider**: Groq
* **Vector Store**: ChromaDB (persistent via Docker volume)
* **Embeddings**: `sentence-transformers/all-MiniLM-L12-v2`
* **PDF Ingestion**: PyPDFLoader + Recursive Text Splitter
* **Python Version**: 3.12.4

---

## ğŸ“ Project Structure

```text
RAG-CHATBOT/
â”œâ”€â”€ client/                 # Streamlit frontend
â”‚   â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ app.py
â”‚   â””â”€â”€ config.py
â”‚
â”œâ”€â”€ server/                 # FastAPI backend
â”‚   â”œâ”€â”€ modules/
â”‚   â”‚   â”œâ”€â”€ load_vectorstore.py
â”‚   â”‚   â”œâ”€â”€ llm.py
â”‚   â”‚   â””â”€â”€ query_handlers.py
â”‚   â”œâ”€â”€ chroma_store/       # Vector DB (Docker volume)
â”‚   â”œâ”€â”€ uploaded_pdfs/      # Uploaded PDFs (runtime)
â”‚   â””â”€â”€ main.py
â”‚
â”œâ”€â”€ logger.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â”œâ”€â”€ .env
â”œâ”€â”€ .gitignore
â””â”€â”€ Dockerfile
```

---

## ğŸš€ Features

* Upload PDFs and build a vector database
* Semantic document retrieval using embeddings
* Context-aware answers powered by Groq LLMs
* Persistent vector storage using Docker volumes
* Clean frontendâ€“backend separation
* Centralized logging and robust error handling

---

## ğŸ”‘ Environment Variables

Create a `.env` file or pass variables at runtime:

```env
GROQ_API_KEY=your_groq_api_key_here
```

---

## ğŸ“¦ Installation (Local)

### 1ï¸âƒ£ Create a Virtual Environment

```bash
python -m venv myenv
source myenv/bin/activate   # macOS / Linux
# myenv\Scripts\activate    # Windows
```

### 2ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

---

## â–¶ï¸ Running Locally

### Start Backend (FastAPI)

```bash
cd server
uvicorn main:app --reload
```

Backend available at:

```
http://127.0.0.1:8000
```

---

### Start Frontend (Streamlit)

```bash
cd client
streamlit run app.py
```

Frontend available at:

```
http://127.0.0.1:8501
```

---

## ğŸ§ª API Endpoints

| Endpoint        | Method | Description                    |
| --------------- | ------ | ------------------------------ |
| `/upload_pdfs/` | POST   | Upload PDF documents           |
| `/ask/`         | POST   | Ask questions to the RAG agent |
| `/test`         | GET    | Health check                   |

---

## ğŸ³ Docker Deployment

### Build Image

```bash
docker build -t rag-chatbot .
```

### Run Container

```bash
docker run -p 8000:8000 -p 8501:8501 \
  -e GROQ_API_KEY=your_groq_api_key_here \
  -v chroma_data:/app/server/chroma_store \
  rag-chatbot
```

### Why Use a Docker Volume?

* Preserves embeddings across container restarts
* Avoids rebuilding the vector database on every run

---

## ğŸ§¹ `.dockerignore` (Recommended)

The following should be excluded from Docker builds:

* Virtual environments
* Cache files
* `.env`
* `chroma_store`
* Uploaded PDFs

This keeps images **secure, lightweight, and reproducible**.

---

## ğŸ“š Key Libraries Used

* LangChain
* ChromaDB
* Sentence Transformers
* Groq SDK
* FastAPI
* Streamlit
* PyPDF

---

## âš ï¸ Notes

* OpenAI models are disabled by default
* Groq models are recommended for inference
* Docker volume is required for persistent vector storage

---

## ğŸ“Œ Future Improvements

* Streaming responses
* Authentication and access control
* Multi-user chat history
* Docker Compose support
* Cloud deployment (AWS / Fly.io / Render)

---

## ğŸ“„ License

MIT License

---
